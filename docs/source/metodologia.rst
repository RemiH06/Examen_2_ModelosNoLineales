Metodolog√≠a
===========

Esta secci√≥n detalla la metodolog√≠a completa utilizada para construir y evaluar los modelos SARIMAX y FFNN.

Pipeline General del Proyecto
------------------------------

.. code-block:: text

   1. Descarga de datos (yfinance)
          ‚Üì
   2. An√°lisis exploratorio (EDA)
          ‚Üì
   3. Pruebas de estacionariedad
          ‚Üì
   4. Preparaci√≥n de datos (train/test/future)
          ‚Üì
   5. Entrenamiento de modelos
          ‚Üì
   6. Evaluaci√≥n y comparaci√≥n
          ‚Üì
   7. Predicciones finales (5 d√≠as)

---

Modelo 1: SARIMAX
-----------------

**SARIMAX** significa *Seasonal AutoRegressive Integrated Moving Average with eXogenous variables*.

Estructura del Modelo
~~~~~~~~~~~~~~~~~~~~~

Un modelo SARIMAX se define por dos conjuntos de √≥rdenes:

**Componente No Estacional: (p, d, q)**

- **p**: Orden autoregresivo (AR) - n√∫mero de lags de la variable dependiente
- **d**: Grado de diferenciaci√≥n (I) - veces que se diferencia la serie
- **q**: Orden de media m√≥vil (MA) - n√∫mero de lags de los errores

**Componente Estacional: (P, D, Q, s)**

- **P**: Orden autoregresivo estacional
- **D**: Diferenciaci√≥n estacional
- **Q**: Media m√≥vil estacional
- **s**: Periodo estacional (e.g., 5 para d√≠as de la semana, 12 para meses)

Ecuaci√≥n General
~~~~~~~~~~~~~~~~

.. math::

   \Phi_P(B^s)\phi_p(B)\nabla^D_s\nabla^d Y_t = \Theta_Q(B^s)\theta_q(B)\epsilon_t

Donde:

- :math:`Y_t`: Serie temporal en el tiempo t
- :math:`B`: Operador de retardo (lag)
- :math:`\nabla`: Operador de diferenciaci√≥n
- :math:`\epsilon_t`: Error o ruido blanco

Selecci√≥n de √ìrdenes
~~~~~~~~~~~~~~~~~~~~

Para determinar los valores √≥ptimos de (p, d, q) y (P, D, Q, s):

1. **Diferenciaci√≥n (d)**
   
   - Aplicar prueba ADF (Augmented Dickey-Fuller)
   - Si p-value > 0.05 ‚Üí serie NO estacionaria ‚Üí aumentar d
   - Confirmar con prueba KPSS

2. **√ìrdenes AR y MA (p, q)**
   
   - Analizar gr√°ficas ACF (Autocorrelation Function)
   - Analizar gr√°ficas PACF (Partial Autocorrelation Function)
   - Reglas emp√≠ricas:
     
     - Si ACF decae lentamente ‚Üí componente AR significativo
     - Si PACF corta en lag k ‚Üí p = k
     - Si ACF corta en lag k ‚Üí q = k

3. **Componente Estacional (P, D, Q, s)**
   
   - Observar patrones repetitivos en ACF/PACF
   - Para acciones: s = 5 (semana burs√°til) o s = 21 (mes aproximado)

4. **Criterios de Informaci√≥n**
   
   - **AIC** (Akaike Information Criterion): penaliza complejidad
   - **BIC** (Bayesian Information Criterion): penaliza m√°s fuertemente
   - Menor AIC/BIC ‚Üí mejor modelo

Configuraci√≥n Final
~~~~~~~~~~~~~~~~~~~

Basado en el an√°lisis exploratorio, seleccionamos:

.. code-block:: python

   order = (1, 1, 1)           # (p, d, q)
   seasonal_order = (1, 1, 1, 5)  # (P, D, Q, s)

**Justificaci√≥n:**

- **d=1**: Una diferenciaci√≥n es suficiente para estacionariedad
- **p=1, q=1**: Patrones simples en ACF/PACF
- **s=5**: Ciclo semanal (5 d√≠as h√°biles)
- **P=1, D=1, Q=1**: Estacionalidad moderada

Criterios de Validaci√≥n
~~~~~~~~~~~~~~~~~~~~~~~~

- ‚úÖ Residuos deben ser ruido blanco (Ljung-Box test)
- ‚úÖ AIC y BIC minimizados
- ‚úÖ Predicciones estables (sin explosi√≥n)

---

Modelo 2: FFNN (Feed-Forward Neural Network)
---------------------------------------------

Una **FFNN** es una red neuronal artificial donde la informaci√≥n fluye en una sola direcci√≥n (sin ciclos).

Arquitectura de la Red
~~~~~~~~~~~~~~~~~~~~~~

Nuestra FFNN tiene la siguiente estructura:

.. code-block:: text

   Input Layer (lookback d√≠as)
         ‚Üì
   Dense(128) + ReLU + L2(0.0001) + Dropout(0.1)
         ‚Üì
   Dense(64) + ReLU + L2(0.0001) + Dropout(0.2)
         ‚Üì
   Dense(32) + ReLU + L2(0.00001) + Dropout(0.1)
         ‚Üì
   Dense(16) + ReLU
         ‚Üì
   Output Layer: Dense(1) - Predicci√≥n del precio

**N√∫mero total de capas**: 5 (4 ocultas + 1 salida)

Hiperpar√°metros Seleccionados
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 30 30 40

   * - Hiperpar√°metro
     - Valor
     - Justificaci√≥n
   * - **Lookback**
     - 20 d√≠as
     - Balance entre memoria hist√≥rica y overfitting
   * - **Capas ocultas**
     - 4
     - Suficiente profundidad sin complejidad excesiva
   * - **Neuronas (L1)**
     - 128
     - Capa ancha para capturar patrones iniciales
   * - **Neuronas (L2)**
     - 64
     - Reducci√≥n progresiva (pir√°mide)
   * - **Neuronas (L3)**
     - 32
     - Extracci√≥n de features de alto nivel
   * - **Neuronas (L4)**
     - 16
     - Consolidaci√≥n final antes de salida
   * - **Funci√≥n de activaci√≥n**
     - ReLU
     - Previene vanishing gradient, computacionalmente eficiente
   * - **Regularizaci√≥n L2**
     - [1e-4, 1e-5, 1e-6]
     - Previene overfitting en pesos grandes
   * - **Dropout**
     - [0.1, 0.2]
     - Desactiva neuronas aleatoriamente (generalizaci√≥n)
   * - **Learning rate**
     - 0.001
     - Tasa est√°ndar para Adam optimizer
   * - **Batch size**
     - 32
     - Balance entre velocidad y estabilidad
   * - **Epochs**
     - 100 (con Early Stopping)
     - Suficiente para convergencia

Funci√≥n de Activaci√≥n: ReLU
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   f(x) = \max(0, x)

**Ventajas:**

- No saturaci√≥n en valores positivos
- Derivada simple (0 o 1)
- Computacionalmente eficiente

Regularizaci√≥n
~~~~~~~~~~~~~~

**1. L2 Regularization (Ridge)**

A√±ade penalizaci√≥n a la funci√≥n de p√©rdida:

.. math::

   L_{total} = L_{MSE} + \lambda \sum_{i} w_i^2

Esto previene que los pesos crezcan excesivamente.

**2. Dropout**

Durante entrenamiento, "apaga" aleatoriamente un porcentaje de neuronas en cada iteraci√≥n.

- **Efecto**: Fuerza redundancia en la red
- **Resultado**: Mejor generalizaci√≥n al test set

Optimizador: Adam
~~~~~~~~~~~~~~~~~

**Adam** (Adaptive Moment Estimation) combina las ventajas de AdaGrad y RMSprop.

**Actualizaci√≥n de pesos:**

.. math::

   m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t
   
   v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
   
   w_t = w_{t-1} - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}

Donde:

- :math:`m_t`: Primer momento (media de gradientes)
- :math:`v_t`: Segundo momento (varianza de gradientes)
- :math:`\alpha`: Learning rate (0.001)
- :math:`\beta_1, \beta_2`: Tasas de decaimiento (0.9, 0.999)

**Ventajas de Adam:**

‚úÖ Adaptaci√≥n individual de learning rate por par√°metro

‚úÖ Funciona bien con gradientes ruidosos

‚úÖ Requiere poca tunning manual

Walk-Forward Validation
~~~~~~~~~~~~~~~~~~~~~~~

Para predicciones de m√∫ltiples d√≠as, usamos un enfoque de **rolling window**:

.. code-block:: text

   D√≠a 1: Usar ventana [t-20:t] ‚Üí Predecir t+1
   D√≠a 2: Usar ventana [t-19:t+1] ‚Üí Predecir t+2
   D√≠a 3: Usar ventana [t-18:t+2] ‚Üí Predecir t+3
   ...

Este m√©todo simula condiciones reales de trading.

Early Stopping
~~~~~~~~~~~~~~

Implementamos Early Stopping para prevenir overfitting:

.. code-block:: python

   EarlyStopping(
       monitor='val_loss',    # Vigilar p√©rdida en validaci√≥n
       patience=15,           # Esperar 15 epochs sin mejora
       restore_best_weights=True  # Regresar a mejor modelo
   )

Si la p√©rdida en validaci√≥n no mejora en 15 epochs consecutivos, el entrenamiento se detiene.

---

Preprocesamiento de Datos
--------------------------

Escalado con MinMaxScaler
~~~~~~~~~~~~~~~~~~~~~~~~~~

Para la FFNN, escalamos los datos al rango [0, 1]:

.. math::

   X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}

**Razones:**

1. Convergencia m√°s r√°pida del optimizador
2. Evita dominancia de features con mayor magnitud
3. Previene problemas num√©ricos (overflow/underflow)

**Importante:** El scaler se ajusta SOLO con datos de entrenamiento para evitar data leakage.

Divisi√≥n de Datos
~~~~~~~~~~~~~~~~~

.. code-block:: python

   Total de datos: ~500 d√≠as (2 a√±os)
   
   Train: 470 d√≠as (94%)
   Test:   30 d√≠as (6%)
   Future:  5 d√≠as (predicci√≥n)

**Train Set:** Para entrenar los modelos

**Test Set:** Para evaluar performance antes de predicci√≥n final

**Future Set:** Predicciones objetivo (20-24 octubre 2025)

Creaci√≥n de Secuencias
~~~~~~~~~~~~~~~~~~~~~~

Para la FFNN, transformamos datos de serie temporal en pares (X, y):

.. code-block:: python

   # Ejemplo con lookback=3
   Precios: [100, 102, 105, 108, 110, 112]
   
   Secuencias:
   X=[100, 102, 105] ‚Üí y=108
   X=[102, 105, 108] ‚Üí y=110
   X=[105, 108, 110] ‚Üí y=112

Esto permite a la red aprender patrones hist√≥ricos.

---

M√©tricas de Evaluaci√≥n
-----------------------

Para comparar ambos modelos, usamos tres m√©tricas est√°ndar:

1. RMSE (Root Mean Squared Error)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}

- **Unidad**: Misma que la variable (d√≥lares)
- **Interpretaci√≥n**: Error promedio en predicciones
- **Sensible a outliers**: Penaliza errores grandes

2. MAE (Mean Absolute Error)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|

- **Unidad**: D√≥lares
- **Interpretaci√≥n**: Error absoluto promedio
- **M√°s robusto** a outliers que RMSE

3. MAPE (Mean Absolute Percentage Error)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   MAPE = \frac{100}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right|

- **Unidad**: Porcentaje (%)
- **Interpretaci√≥n**: Error relativo promedio
- **Ventaja**: Independiente de la escala del precio

Criterios de √âxito
~~~~~~~~~~~~~~~~~~

Un modelo se considera "bueno" si:

- ‚úÖ RMSE < $10 (para acciones ~$100-500)
- ‚úÖ MAE < $7
- ‚úÖ MAPE < 5%
- ‚úÖ Predicciones estables (sin saltos abruptos)

---

Reproducibilidad
----------------

Para garantizar que los resultados sean reproducibles:

Semilla Fija
~~~~~~~~~~~~

.. code-block:: python

   SEED = 42
   
   np.random.seed(SEED)
   tf.random.set_seed(SEED)
   random.seed(SEED)

Esto asegura que:

- Divisi√≥n train/test sea id√©ntica
- Inicializaci√≥n de pesos de FFNN sea consistente
- Dropout y shuffle sean determin√≠sticos

Control de Versiones
~~~~~~~~~~~~~~~~~~~~~

Todas las dependencias tienen versiones fijas en ``requirements.txt``:

.. code-block:: text

   tensorflow==2.13.0
   statsmodels==0.14.0
   numpy==1.24.3
   ...

Entorno de Ejecuci√≥n
~~~~~~~~~~~~~~~~~~~~

- **Python**: 3.8+
- **Sistema**: Compatible con Linux, macOS, Windows
- **Hardware**: CPU suficiente (GPU opcional para FFNN)

---

Limitaciones y Consideraciones
-------------------------------

Limitaciones del Proyecto
~~~~~~~~~~~~~~~~~~~~~~~~~~

1. **Horizonte corto**: Solo 5 d√≠as (dificultad aumenta exponencialmente)
2. **Datos hist√≥ricos √∫nicamente**: No incorpora noticias o sentimiento
3. **Eventos imprevistos**: No puede anticipar anuncios sorpresa
4. **Suposici√≥n de mercado eficiente**: Los patrones del pasado pueden cambiar

Supuestos del Modelo
~~~~~~~~~~~~~~~~~~~~

- üìä Los datos hist√≥ricos contienen informaci√≥n predictiva
- üìÖ Los patrones temporales son relativamente estables
- üíπ No hay cambios estructurales dr√°sticos durante predicci√≥n
- üîÑ Los mercados operan normalmente (no crisis inesperadas)

Consideraciones √âticas
~~~~~~~~~~~~~~~~~~~~~~~

- ‚ö†Ô∏è Este proyecto es **acad√©mico**, no constituye asesor√≠a financiera
- ‚ö†Ô∏è No debe usarse para decisiones reales de inversi√≥n sin an√°lisis adicional
- ‚ö†Ô∏è Los mercados financieros son inherentemente impredecibles

---

Resumen de la Metodolog√≠a
--------------------------

.. list-table::
   :header-rows: 1
   :widths: 25 35 40

   * - Aspecto
     - SARIMAX
     - FFNN
   * - **Tipo de modelo**
     - Estad√≠stico param√©trico
     - Aprendizaje profundo
   * - **Hiperpar√°metros clave**
     - (p,d,q), (P,D,Q,s)
     - Capas, neuronas, dropout, LR
   * - **Preprocesamiento**
     - Diferenciaci√≥n
     - Escalado MinMax
   * - **Ventajas**
     - Interpretable, te√≥ricamente fundamentado
     - Captura no linealidades complejas
   * - **Desventajas**
     - Asume linealidad, estacionariedad
     - Caja negra, requiere muchos datos
   * - **Tiempo de entrenamiento**
     - R√°pido (~segundos)
     - Moderado (~minutos)

Ambos modelos son complementarios y su comparaci√≥n permite evaluar si m√©todos estad√≠sticos cl√°sicos o deep learning son m√°s efectivos para este problema espec√≠fico.